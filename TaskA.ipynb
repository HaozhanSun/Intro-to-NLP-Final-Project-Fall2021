{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "TaskA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee91a112d4d64fc4acbabab4dc73c9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_105c35ffaca0443590db5313230872dd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7483aa5431e441c28a4ebb5f392f64fd",
              "IPY_MODEL_60391a1f3da541a5b9789892f3f70233",
              "IPY_MODEL_e59602e6235a4a638ae3265cd92b05a4"
            ]
          }
        },
        "105c35ffaca0443590db5313230872dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7483aa5431e441c28a4ebb5f392f64fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fedc20a8b171440ca9a0afb7d49b5aef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "split=train : ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33b19715ddf841a981c771dca3eea8e2"
          }
        },
        "60391a1f3da541a5b9789892f3f70233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4c67d450e74444449ff444349e9eaa69",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 277,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 277,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b04470dbb5f4b9ea53c141debf9a6fe"
          }
        },
        "e59602e6235a4a638ae3265cd92b05a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eb49044a05ca4191a93a45b4fc60ea33",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 278/? [01:04&lt;00:00,  4.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7eb44d9eb1e414d9c7c98742bcff25d"
          }
        },
        "fedc20a8b171440ca9a0afb7d49b5aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33b19715ddf841a981c771dca3eea8e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c67d450e74444449ff444349e9eaa69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b04470dbb5f4b9ea53c141debf9a6fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb49044a05ca4191a93a45b4fc60ea33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7eb44d9eb1e414d9c7c98742bcff25d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e64efcc06f2942aeb0359e9361179c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3c8cff59880545568558bd28ed788f2e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a48ebf68989e4cd1abf3bdd493896e85",
              "IPY_MODEL_75d318b9d92a42c4bde9b5a510984550",
              "IPY_MODEL_d6df6c4c1c8d46bdb0c442a1bb6b3985"
            ]
          }
        },
        "3c8cff59880545568558bd28ed788f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a48ebf68989e4cd1abf3bdd493896e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9f716ada6e844c668b061b0945d0cade",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80a7b23d9c68421491937529f5d59217"
          }
        },
        "75d318b9d92a42c4bde9b5a510984550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8ed9df3a0d034e95ba1b6b9bffb9d583",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b22b11e2dead41caa69776c1569b50cd"
          }
        },
        "d6df6c4c1c8d46bdb0c442a1bb6b3985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c440811255444160b8da47e3ed86b808",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [03:14&lt;00:00, 64.88s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bfa4e8380ac24fc79fbc37d2129b650e"
          }
        },
        "9f716ada6e844c668b061b0945d0cade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80a7b23d9c68421491937529f5d59217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ed9df3a0d034e95ba1b6b9bffb9d583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b22b11e2dead41caa69776c1569b50cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c440811255444160b8da47e3ed86b808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bfa4e8380ac24fc79fbc37d2129b650e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnXQrkMlG2ga"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9_adIlNl3bJ"
      },
      "source": [
        "!pip install transformers==2.7.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jknbt-9FG9sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1680106a-01d8-4cfd-94a7-09416a117350"
      },
      "source": [
        "!git clone https://github.com/KennethSunn/Intro-to-NLP-Final-Project-Fall2021.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Intro-to-NLP-Final-Project-Fall2021'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 60 (delta 13), reused 37 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmMKCJ-dG2ge"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/Intro-to-NLP-Final-Project-Fall2021/src/')\n",
        "import collections\n",
        "from typing import Callable\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "from tqdm import notebook\n",
        "import importlib\n",
        "import pprint\n",
        "import nltk\n",
        "import datetime\n",
        "import os\n",
        "from argparse import Namespace\n",
        "import re\n",
        "from collections import Counter"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWwTBxXqG2gh"
      },
      "source": [
        "import utils.general as general_utils\n",
        "import utils.transformer.data as transformer_data_utils\n",
        "import utils.transformer.general as transformer_general_utils\n",
        "general_utils.set_seed_everywhere() #set the seed for reproducibility"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EfdINgcG2gk"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO) "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6k1nQ5d4G2gn"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcFCAJAQG2gr"
      },
      "source": [
        "## Import Optimzer and transformers Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc66UeiyG2gs"
      },
      "source": [
        "# Import RAdam and Lookahead\n",
        "from radam.radam import RAdam\n",
        "from lookahead.optimizer import Lookahead"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "sF7KP8tzG2gv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dfb9f8b-0217-46bb-d4d0-f1e4490fff66"
      },
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from transformers import AlbertTokenizer, AlbertModel\n",
        "from transformers import RobertaTokenizer, RobertaModel"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:transformers.file_utils:PyTorch version 1.10.0+cu111 available.\n",
            "INFO:transformers.file_utils:TensorFlow version 2.7.0 available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIAYRiqhG2gz"
      },
      "source": [
        "# Set up the argspace/important_variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F6tx_VrG2gz"
      },
      "source": [
        "args = Namespace(\n",
        "        #use cuda by default\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \n",
        "        #set batch size and number of epochs\n",
        "        batch_size = 32,\n",
        "        num_epochs = 10,\n",
        "    \n",
        "        #set the learning rate\n",
        "        learning_rate = 0.0001,\n",
        "\n",
        "        #location of the train, dev and test csv\n",
        "        train_val_csv = '/content/Intro-to-NLP-Final-Project-Fall2021/TaskA_data/train_a.csv',\n",
        "        test_csv = '/content/Intro-to-NLP-Final-Project-Fall2021/TaskA_data/test_a_tweets.tsv',\n",
        "        test_label = '/content/Intro-to-NLP-Final-Project-Fall2021/TaskA_data/test_a_labels.csv',\n",
        "        #directory to save our models at\n",
        "        directory = './', \n",
        "        model_name = 'distilgpt2.pt',\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY29sw-7G2g2"
      },
      "source": [
        "## Loading a presplit subset of the full dataset data into DataFrames. \n",
        "Here, 0 : NOT, 1 : HOF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oTR-SaSYIjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5763777-9b88-4603-c4f7-b88d561a171e"
      },
      "source": [
        "label = pd.read_csv(args.test_label, header=None, names=['id', 'label'])\n",
        "map_of_label = {'OFF': 1, 'NOT': 0}\n",
        "label['label'] = label['label'].map(map_of_label)\n",
        "label['label'].value_counts()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2807\n",
              "1    1080\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KeZJS7HTO5a"
      },
      "source": [
        "test_set = pd.read_csv(args.test_csv, sep='\\t')\n",
        "test_set.rename(columns={'tweet': 'text'}, inplace=True)\n",
        "test_set['split'] = 'test'\n",
        "test_set['label'] = label['label'].values"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKKJ-JmZG2g3"
      },
      "source": [
        "data_df =  pd.read_csv(args.train_val_csv)\n",
        "data_df.drop(columns=['average', 'std'], inplace=True)\n",
        "data_df.drop(data_df[data_df['split'] == 'test'].index, inplace = True)\n",
        "data_df = pd.concat([data_df, test_set])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKy-82YrG2hh"
      },
      "source": [
        "## Create the text preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyhSisD5G2hi"
      },
      "source": [
        "class GPT2Preprocessor:\n",
        "    def __init__(self, transformer_tokenizer, sentence_detector):\n",
        "        self.transformer_tokenizer = transformer_tokenizer\n",
        "        self.sentence_detector = sentence_detector\n",
        "\n",
        "    def add_eos_tokens(self, text):\n",
        "        eos_token = \" \" + self.transformer_tokenizer.eos_token + \" \"\n",
        "        sentences = self.sentence_detector.tokenize(text)\n",
        "        eos_added_text = (\n",
        "            eos_token.join(sentences) + \" \" + self.transformer_tokenizer.eos_token\n",
        "        )\n",
        "        return eos_added_text\n",
        "\n",
        "class AlbertPreprocessor:\n",
        "    def __init__(self, transformer_tokenizer, sentence_detector):\n",
        "        self.transformer_tokenizer = transformer_tokenizer\n",
        "        self.sentence_detector = sentence_detector\n",
        "\n",
        "    def add_eos_tokens(self, text):\n",
        "        eos_token = \" \" + self.transformer_tokenizer.eos_token + \" \"\n",
        "        sentences = self.sentence_detector.tokenize(text)\n",
        "        eos_added_text = (\n",
        "            eos_token.join(sentences) + \" \" + self.transformer_tokenizer.eos_token\n",
        "        )\n",
        "        return eos_added_text\n",
        "\n",
        "class RobertaPreprocessor:\n",
        "    def __init__(self, transformer_tokenizer, sentence_detector):\n",
        "        self.transformer_tokenizer = transformer_tokenizer\n",
        "        self.sentence_detector = sentence_detector\n",
        "\n",
        "    def add_eos_tokens(self, text):\n",
        "        eos_token = \" \" + self.transformer_tokenizer.eos_token + \" \"\n",
        "        sentences = self.sentence_detector.tokenize(text)\n",
        "        eos_added_text = (\n",
        "            eos_token.join(sentences) + \" \" + self.transformer_tokenizer.eos_token\n",
        "        )\n",
        "        return eos_added_text"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGCMVmYNJHca"
      },
      "source": [
        "!python -c 'import nltk; nltk.download(\"punkt\")'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uX6GH1WNG2hl"
      },
      "source": [
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "albert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
        "#roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "punkt_sentence_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZCJQllG2hn"
      },
      "source": [
        "gpt2_preproc = GPT2Preprocessor(gpt2_tokenizer, punkt_sentence_detector)\n",
        "albert_preproc = AlbertPreprocessor(albert_tokenizer, punkt_sentence_detector)\n",
        "roberta_preproc = RobertaPreprocessor(roberta_tokenizer, punkt_sentence_detector)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4JACIdi9bWJ"
      },
      "source": [
        "#add the special tokens\n",
        "data_df[\"text\"] = data_df[\"text\"].map(gpt2_preproc.add_eos_tokens)\n",
        "#data_df[\"text\"] = data_df[\"text\"].map(albert_preproc.add_eos_tokens)\n",
        "#data_df[\"text\"] = data_df[\"text\"].map(roberta_preproc.add_eos_tokens)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43q7GSYEG2hu"
      },
      "source": [
        "### Create the torch torch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlI6CoLfG2hv"
      },
      "source": [
        "dataset = transformer_data_utils.HateDataset(\n",
        "         data_df=data_df, tokenizer=gpt2_tokenizer\n",
        "     )\n",
        "#dataset = transformer_data_utils.HateDataset(\n",
        "#        data_df=data_df, tokenizer=albert_tokenizer\n",
        "#    )\n",
        "#dataset = transformer_data_utils.HateDataset(\n",
        "#    data_df=data_df, tokenizer=roberta_tokenizer \n",
        "#)\n",
        "assert dataset._max_seq_length <= 512"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYtRKnWClqBE",
        "outputId": "9971b925-e413-405f-b4e2-a88bd90be60e"
      },
      "source": [
        "print(dataset.__getitem__(0)['x_data'].shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(349,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZsYMvrI_da7"
      },
      "source": [
        "# Creating the Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYFlt6Au_F1d"
      },
      "source": [
        "class SimpleGPT2SequenceClassifier(nn.Module):\n",
        "     def __init__(\n",
        "         self, \n",
        "         hidden_size: int,\n",
        "         num_classes:int ,\n",
        "         max_seq_len:int,\n",
        "         gpt_model_name:str, \n",
        "     ):\n",
        "         super(SimpleGPT2SequenceClassifier,self).__init__()\n",
        "         self.gpt2model = GPT2Model.from_pretrained(\n",
        "             gpt_model_name\n",
        "         )\n",
        "         self.fc1 = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "     def forward(self, x_in):\n",
        "         gpt_out = self.gpt2model(x_in)[0] #returns tuple\n",
        "         batch_size = gpt_out.shape[0]\n",
        "         prediction_vector = self.fc1(gpt_out.view(batch_size,-1)) #(batch_size , max_len, num_classes)\n",
        "         return prediction_vector\n",
        "\n",
        "class SimpleRobertaSequenceClassifier(nn.Module):\n",
        "     def __init__(\n",
        "         self, \n",
        "         hidden_size: int,\n",
        "         num_classes:int ,\n",
        "         max_seq_len:int,\n",
        "         roberta_model_name:str, \n",
        "     ):\n",
        "         super(SimpleRobertaSequenceClassifier,self).__init__()\n",
        "         self.robertamodel = RobertaModel.from_pretrained(\n",
        "             roberta_model_name\n",
        "         )\n",
        "         self.fc1 = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "     def forward(self, x_in):\n",
        "         gpt_out = self.robertamodel(x_in)[0] #returns tuple\n",
        "         batch_size = gpt_out.shape[0]\n",
        "         prediction_vector = self.fc1(gpt_out.view(batch_size,-1)) #(batch_size , max_len, num_classes)\n",
        "         return prediction_vector\n",
        "\n",
        "class SimpleAlbertSequenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        hidden_size: int,\n",
        "        num_classes:int ,\n",
        "        max_seq_len:int,\n",
        "        albert_model_name:str, \n",
        "    ):\n",
        "        super(SimpleAlbertSequenceClassifier,self).__init__()\n",
        "        self.albertmodel = AlbertModel.from_pretrained(\n",
        "            albert_model_name\n",
        "        )\n",
        "        self.fc1 = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x_in):\n",
        "        albert_out = self.albertmodel(x_in)[0] #returns tuple\n",
        "        batch_size = albert_out.shape[0]\n",
        "        prediction_vector = self.fc1(albert_out.view(batch_size,-1)) #(batch_size , max_len, num_classes)\n",
        "        return prediction_vector"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4MGsBSm_oXR"
      },
      "source": [
        "print(\"Loading Pretrained distilgpt2...\")\n",
        "#print(\"Loading Pretrained albert...\")\n",
        "#print('Loading Pretrained roberta...')\n",
        "num_classes = len(set(data_df.label))\n",
        "hidden_size = dataset._max_seq_length * 768\n",
        "model = SimpleGPT2SequenceClassifier(\n",
        "     hidden_size=hidden_size,\n",
        "     num_classes=num_classes,\n",
        "     gpt_model_name=\"distilgpt2\",\n",
        "     max_seq_len=dataset._max_seq_length,\n",
        ")\n",
        "#model = SimpleAlbertSequenceClassifier(\n",
        "#    hidden_size=hidden_size,\n",
        "#    num_classes=num_classes,\n",
        "#    albert_model_name='albert-base-v2',\n",
        "#    max_seq_len=dataset._max_seq_length,\n",
        "#)\n",
        "#model = SimpleRobertaSequenceClassifier(\n",
        "#    hidden_size=hidden_size,\n",
        "#    num_classes=num_classes,\n",
        "#    max_seq_len=dataset._max_seq_length,\n",
        "#    roberta_model_name=\"distilroberta-base\"\n",
        "#)\n",
        "model.to(args.device)\n",
        "print(\"Finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLAYZmmtG2iJ"
      },
      "source": [
        "# Begin Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtX_A6l0Aq6r"
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "base_optimizer = RAdam(model.parameters(), lr=args.learning_rate)\n",
        "optimizer = Lookahead(optimizer=base_optimizer, k=5, alpha=0.5)\n",
        "args.num_epochs = 10"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vC3ewUSnG2iK"
      },
      "source": [
        "train_state = general_utils.make_train_state()\n",
        "train_state[\"ckpt\"] = 0\n",
        "train_state['max_seq_len'] = dataset._max_seq_length"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc81HVEZKWzO"
      },
      "source": [
        "args.batch_size = 14\n",
        "early_stopping = transformer_general_utils.EarlyStopping(patience=4)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "qg3U6LcmG2iN"
      },
      "source": [
        "epoch_bar = notebook.tqdm(\n",
        "    desc = 'training_routine',\n",
        "    total = args.num_epochs,\n",
        "    position=0,\n",
        "    leave = True,\n",
        ")\n",
        "dataset.set_split('train')\n",
        "train_bar = notebook.tqdm(\n",
        "    desc = 'split=train ',\n",
        "    total=dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "dataset.set_split('val')\n",
        "eval_bar = notebook.tqdm(\n",
        "    desc = 'split=eval',\n",
        "    total=dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "\n",
        "for epoch_index in range(args.num_epochs):\n",
        "    train_state['epoch_in'] = epoch_index\n",
        "\n",
        "    dataset.set_split('train')\n",
        "    batch_generator = transformer_data_utils.generate_batches(\n",
        "        dataset=dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        device=args.device,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    running_f1 = 0.0\n",
        "    model.train()\n",
        "\n",
        "    train_bar.reset(\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    model.train()\n",
        "    for batch_index, batch_dict in enumerate(batch_generator):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(batch_dict[\"x_data\"])\n",
        "\n",
        "        loss = loss_func(y_pred, batch_dict[\"y_target\"])\n",
        "        loss_t = loss.item()\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "                             \n",
        "        loss_t = loss.item()\n",
        "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                             \n",
        "        y_pred = y_pred.detach().cpu()\n",
        "\n",
        "        batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
        "        \n",
        "        acc_t = transformer_general_utils.compute_accuracy(\n",
        "            y_pred, batch_dict[\"y_target\"]\n",
        "        )\n",
        "\n",
        "        f1_t = transformer_general_utils.compute_macro_f1(\n",
        "            y_pred, batch_dict[\"y_target\"]\n",
        "        )\n",
        "\n",
        "        train_state[\"batch_preds\"].append(y_pred)\n",
        "        train_state[\"batch_targets\"].append(batch_dict[\"y_target\"])\n",
        "        train_state[\"batch_indexes\"].append(batch_dict[\"x_index\"])\n",
        "\n",
        "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "        running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "\n",
        "        train_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
        "                             epoch=epoch_index)\n",
        "\n",
        "        train_bar.update()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    train_state['train_accuracies'].append(running_acc)\n",
        "    train_state['train_losses'].append(running_loss)\n",
        "    \n",
        "    train_state['train_preds'].append(\n",
        "        torch.cat(train_state['batch_preds']).cpu()\n",
        "    )\n",
        "    train_state['train_targets'].append(\n",
        "        torch.cat(train_state['batch_targets']).cpu()\n",
        "    )\n",
        "    train_state['train_indexes'].append(\n",
        "        torch.cat(train_state['batch_indexes']).cpu()\n",
        "    )\n",
        "    train_f1 = transformer_general_utils \\\n",
        "                .compute_macro_f1(train_state['train_preds'][-1],\n",
        "                                  train_state['train_targets'][-1],\n",
        "                                 )\n",
        "                                 \n",
        "    train_state['train_f1s'].append(train_f1)\n",
        "    \n",
        "    train_state['batch_preds'] = []\n",
        "    train_state['batch_targets'] = []\n",
        "    train_state['batch_indexes'] = []\n",
        "    \n",
        "    \n",
        "    dataset.set_split('val')\n",
        "    batch_generator = transformer_data_utils.generate_batches(\n",
        "        dataset= dataset, batch_size= args.batch_size, shuffle=True,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = False, n_workers = 2, \n",
        "    )\n",
        "    eval_bar.reset(\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    running_f1 = 0.0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        optimizer._backup_and_load_cache()\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            y_pred = model(batch_dict[\"x_data\"])\n",
        "\n",
        "            loss = loss_func(y_pred, batch_dict[\"y_target\"])\n",
        "            loss_t = loss.item()\n",
        "\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            y_pred = y_pred.detach()\n",
        "            \n",
        "            acc_t = transformer_general_utils.compute_accuracy(\n",
        "                y_pred, batch_dict[\"y_target\"]\n",
        "            )\n",
        "\n",
        "            f1_t = transformer_general_utils.compute_macro_f1(\n",
        "                y_pred, batch_dict[\"y_target\"]\n",
        "            )\n",
        "\n",
        "            train_state['batch_preds'].append(y_pred.cpu())\n",
        "            train_state['batch_targets'].append(batch_dict['y_target'])\n",
        "            train_state['batch_indexes'].append(batch_dict['x_index'].cpu())\n",
        "\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "            running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "            \n",
        "\n",
        "            eval_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
        "                                 epoch=epoch_index)\n",
        "            eval_bar.update()\n",
        "            \n",
        "    train_state['val_accuracies'].append(running_acc)\n",
        "    train_state['val_losses'].append(running_loss)\n",
        "    \n",
        "        \n",
        "    train_state['val_preds'].append(\n",
        "        torch.cat(train_state['batch_preds']).cpu()\n",
        "    )\n",
        "\n",
        "    train_state['val_targets'].append(\n",
        "        torch.cat(train_state['batch_targets']).cpu()\n",
        "    )\n",
        "\n",
        "    train_state['val_indexes'].append(\n",
        "        torch.cat(train_state['batch_indexes']).cpu()\n",
        "    )\n",
        "\n",
        "    val_f1 = transformer_general_utils \\\n",
        "                .compute_macro_f1(train_state['val_preds'][-1],\n",
        "                                  train_state['val_targets'][-1],\n",
        "                                 )\n",
        "          \n",
        "    train_state['val_f1s'].append(val_f1)\n",
        "    \n",
        "    train_state['batch_preds'] = []\n",
        "    train_state['batch_targets'] = []\n",
        "    train_state['batch_indexes'] = []\n",
        "    \n",
        "    torch.save(\n",
        "        {\n",
        "            'model':model.state_dict(),\n",
        "        },\n",
        "        args.directory + f'_epoc_{epoch_index}_' + args.model_name,\n",
        "    )\n",
        "    \n",
        "    optimizer._clear_and_load_backup()\n",
        "    epoch_bar.set_postfix(best_f1 = early_stopping.best_score, current = val_f1)\n",
        "    epoch_bar.update()\n",
        "\n",
        "    epoch_bar.set_postfix(best_f1 = early_stopping.best_score, current = val_f1)\n",
        "    epoch_bar.update()\n",
        "    print(epoch_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "X3cbb-1dG2iQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba852a39-190e-4dc1-f35f-a3d5cada9892"
      },
      "source": [
        "print(train_state['train_f1s'])"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8171802897875236]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoYjyEe1G2iS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b872614-5c8e-4d4b-9860-675bb7884156"
      },
      "source": [
        "print(train_state['val_f1s'])"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.909488974752727]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJePw9boG2iU"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrQhfw2iG2iY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f6c691-bc64-44ab-ae3f-ca89a570fa8c"
      },
      "source": [
        "best_run_index = train_state['val_f1s'].index(max(train_state['val_f1s']))\n",
        "print(f'Best run at epoch {best_run_index}')\n",
        "print('Train:',classification_report(\n",
        "    y_pred=(torch.argmax(train_state['train_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['train_targets'][best_run_index].cpu().numpy(), \n",
        "    digits=4)\n",
        ")\n",
        "print('Dev:',classification_report(\n",
        "    y_pred=(torch.argmax(train_state['val_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['val_targets'][best_run_index].cpu().numpy(), \n",
        "    digits=4)\n",
        ")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best run at epoch 0\n",
            "Train:               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9355    0.9543    0.9448     53481\n",
            "           1     0.7306    0.6529    0.6895     10141\n",
            "\n",
            "    accuracy                         0.9063     63622\n",
            "   macro avg     0.8330    0.8036    0.8172     63622\n",
            "weighted avg     0.9028    0.9063    0.9041     63622\n",
            "\n",
            "Dev:               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9699    0.9729    0.9714     11460\n",
            "           1     0.8545    0.8408    0.8476      2173\n",
            "\n",
            "    accuracy                         0.9518     13633\n",
            "   macro avg     0.9122    0.9068    0.9095     13633\n",
            "weighted avg     0.9515    0.9518    0.9516     13633\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI1XaxsPG2ib"
      },
      "source": [
        "## Checkpoint ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOEv36m1G2ib"
      },
      "source": [
        "def sort_preds(indexes, preds):\n",
        "    \"\"\"Sorts the predictions in order, to reverse the effects of shuffle\n",
        "    done by dataloader\"\"\"\n",
        "    indexes = indexes.cpu().numpy().reshape(-1,1)\n",
        "    preds = preds.cpu().numpy()\n",
        "    arr_concat = np.hstack((indexes,preds)) #concat the preds and their indexes\n",
        "    sort_arr = arr_concat[ arr_concat[:,0].argsort()] #sort based on the indexes\n",
        "    sorted_preds = np.delete(sort_arr,0,axis=1)\n",
        "    return sorted_preds\n",
        "\n",
        "def get_optimal_models(train_state, split, reverse=False ):\n",
        "    \"\"\"Naive Ensembling\"\"\"\n",
        "    trgts= sort_preds(train_state[f'{split}_indexes'][-1],train_state[f'{split}_targets'][-1].reshape(-1,1))\n",
        "    total_preds = len(train_state[f'{split}_indexes'])\n",
        "    init = np.zeros(train_state[f'{split}_preds'][-1].shape)\n",
        "    max_f1 = 0\n",
        "    idxes = []\n",
        "    rng = range(0,total_preds)\n",
        "    if reverse:\n",
        "        rng = reversed(rng)\n",
        "    for i in rng:\n",
        "        temp = sort_preds(train_state[f'{split}_indexes'][i],train_state[f'{split}_preds'][i])\n",
        "        temp2 = init+temp\n",
        "        f1 = f1_score(\n",
        "            y_pred=temp2.argmax(axis=1),\n",
        "            y_true= trgts, average ='weighted'\n",
        "        )\n",
        "        if f1 > max_f1:\n",
        "            max_f1 = f1\n",
        "            init = init+temp\n",
        "            idxes.append(i)\n",
        "    print(f'Taking preds from {idxes} | Dev f1:{f1}')\n",
        "    return (idxes,max_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDBXq6bCX5P_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1dc60d2-430f-40ea-d8e4-7cef9145ea47"
      },
      "source": [
        "train_state['val_f1s']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4567010720121149,\n",
              " 0.4567010720121149,\n",
              " 0.4567010720121149,\n",
              " 0.4567010720121149,\n",
              " 0.4567010720121149,\n",
              " 0.4567010720121149,\n",
              " 0.8359722219773102,\n",
              " 0.8640951104943737,\n",
              " 0.8930587491997887,\n",
              " 0.8915854573179369]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vWIVkH1yG2ie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3b9121-61b2-49d9-8e21-18faf2dd4a31"
      },
      "source": [
        "best_model_f1_score = f1_score(\n",
        "    y_pred=(torch.argmax(train_state['val_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['val_targets'][best_run_index].cpu().numpy(), \n",
        "    average='weighted'\n",
        ")\n",
        "_models= [get_optimal_models(train_state,'val', reverse=False),\n",
        "                 get_optimal_models(train_state,'val', reverse=True),\n",
        "                 ([best_run_index],best_model_f1_score),]\n",
        "optimal_models = max(_models, key=lambda x:x[1]) #select ensembles or best model \n",
        "print(f'Optimal models chosen: {optimal_models}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taking preds from [0, 6, 7, 8, 9] | Dev f1:0.9448525010850208\n",
            "Taking preds from [9, 8, 7] | Dev f1:0.9473387834097712\n",
            "Optimal models chosen: ([9, 8, 7], 0.9484107144067528)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpjIrLT7G2ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6f3b7f-c844-4333-9287-83387008a9e0"
      },
      "source": [
        "all_models= [os.path.join(args.directory,i) for i in os.listdir(args.directory) if args.model_name in i]\n",
        "all_models = sorted(all_models, key = lambda x: int(x[8])) #sort by epoch num.\n",
        "all_models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./_epoc_0_distilgpt2.pt',\n",
              " './_epoc_1_distilgpt2.pt',\n",
              " './_epoc_2_distilgpt2.pt',\n",
              " './_epoc_3_distilgpt2.pt',\n",
              " './_epoc_4_distilgpt2.pt',\n",
              " './_epoc_5_distilgpt2.pt',\n",
              " './_epoc_6_distilgpt2.pt',\n",
              " './_epoc_7_distilgpt2.pt',\n",
              " './_epoc_8_distilgpt2.pt',\n",
              " './_epoc_9_distilgpt2.pt']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fya481n6G2im",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da00d68-ccb5-4a5a-99c5-b04c9dde0ccf"
      },
      "source": [
        "selected_models = [all_models[i] for i in optimal_models[0]]\n",
        "pprint.pprint(selected_models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['./_epoc_9_distilgpt2.pt',\n",
            " './_epoc_8_distilgpt2.pt',\n",
            " './_epoc_7_distilgpt2.pt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLdc2BW3G2iq"
      },
      "source": [
        "## Loading test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw8BEY2Sbt7I"
      },
      "source": [
        "test_dataset = dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DoISn3hG2i3"
      },
      "source": [
        "test_dataset.set_split('test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqvg3wXKhOgv"
      },
      "source": [
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=False, device=\"cpu\", pinned_memory = False, n_workers = 0): \n",
        "    \"\"\"\n",
        "    A generator function which wraps the PyTorch DataLoader. It will \n",
        "      ensure each tensor is on the write device location.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last,\n",
        "                            pin_memory= pinned_memory,\n",
        "                            num_workers = n_workers,\n",
        "                            )\n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        # print(data_dict.items())\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device, non_blocking= (True if pinned_memory else False) )\n",
        "        yield out_data_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5AQRrWaG2i6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "ee91a112d4d64fc4acbabab4dc73c9c9",
            "105c35ffaca0443590db5313230872dd",
            "7483aa5431e441c28a4ebb5f392f64fd",
            "60391a1f3da541a5b9789892f3f70233",
            "e59602e6235a4a638ae3265cd92b05a4",
            "fedc20a8b171440ca9a0afb7d49b5aef",
            "33b19715ddf841a981c771dca3eea8e2",
            "4c67d450e74444449ff444349e9eaa69",
            "9b04470dbb5f4b9ea53c141debf9a6fe",
            "eb49044a05ca4191a93a45b4fc60ea33",
            "e7eb44d9eb1e414d9c7c98742bcff25d",
            "e64efcc06f2942aeb0359e9361179c1a",
            "3c8cff59880545568558bd28ed788f2e",
            "a48ebf68989e4cd1abf3bdd493896e85",
            "75d318b9d92a42c4bde9b5a510984550",
            "d6df6c4c1c8d46bdb0c442a1bb6b3985",
            "9f716ada6e844c668b061b0945d0cade",
            "80a7b23d9c68421491937529f5d59217",
            "8ed9df3a0d034e95ba1b6b9bffb9d583",
            "b22b11e2dead41caa69776c1569b50cd",
            "c440811255444160b8da47e3ed86b808",
            "bfa4e8380ac24fc79fbc37d2129b650e"
          ]
        },
        "outputId": "165c63af-c733-450e-ee8c-282fcf27263d"
      },
      "source": [
        "test_state = general_utils.make_train_state() \n",
        "test_dataset.set_split('test')\n",
        "eval_bar = notebook.tqdm(\n",
        "    desc = 'split=train ',\n",
        "    total=test_dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "model.eval()\n",
        "for m in notebook.tqdm(selected_models, total=len(selected_models)):\n",
        "    eval_bar.reset(\n",
        "        total=test_dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    model.load_state_dict(torch.load(m)['model'])\n",
        "    batch_generator = generate_batches(\n",
        "        dataset= test_dataset, batch_size= args.batch_size, shuffle=False,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = True, n_workers = 1, \n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "            y_pred = model(batch_dict[\"x_data\"])\n",
        "\n",
        "            y_pred = y_pred.detach()\n",
        "            \n",
        "            batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
        "            test_state['batch_preds'].append(y_pred.cpu())\n",
        "            test_state['batch_targets'].append(batch_dict['y_target'].cpu())\n",
        "            test_state['batch_indexes'].append(batch_dict['x_index'].cpu())\n",
        "            eval_bar.update()\n",
        "\n",
        "    test_state['val_preds'].append(\n",
        "        torch.cat(test_state['batch_preds']).cpu()\n",
        "    )\n",
        "    test_state['val_targets'].append(\n",
        "        torch.cat(test_state['batch_targets']).cpu()\n",
        "    )\n",
        "    test_state['val_indexes'].append(\n",
        "        torch.cat(test_state['batch_indexes']).cpu()\n",
        "    )\n",
        "    \n",
        "    test_state['batch_preds'] = []\n",
        "    test_state['batch_targets'] = []\n",
        "    test_state['batch_indexes'] = []\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee91a112d4d64fc4acbabab4dc73c9c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "split=train :   0%|          | 0/277 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e64efcc06f2942aeb0359e9361179c1a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBLNs164G2i7"
      },
      "source": [
        "assert len(test_state['val_preds']) == len(optimal_models[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tPSiWo8bjPR"
      },
      "source": [
        "### Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiiCZ-VyG2i-"
      },
      "source": [
        "ensemble = torch.zeros_like(test_state['val_preds'][-1])\n",
        "for i in test_state['val_preds']:\n",
        "    ensemble += i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqINUy7gG2jD"
      },
      "source": [
        "test_preds = torch.argmax(ensemble, dim=1).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHaiMjbSNxqk"
      },
      "source": [
        "#test_df = data_df.loc[data_df['split'] == 'test']\n",
        "y_true = np.array(data_df.loc[data_df['split'] == 'test']['label'])\n",
        "y_pred = np.array(test_preds)\n",
        "with open('y_pred_gpt2.npy', 'wb') as f:\n",
        "  np.save(f, y_pred)\n",
        "from sklearn.metrics import f1_score\n",
        "test_acc = accuracy_score(y_pred, y_true)\n",
        "test_f1 = f1_score(y_pred, y_true, average='macro')\n",
        "print(test_acc)\n",
        "print(test_f1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}